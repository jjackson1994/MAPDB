{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f76699",
   "metadata": {},
   "source": [
    "# Continuing on the Dask Data structures\n",
    "\n",
    "Dask offers several pythonic data structures to handle and operate with larger-than-memory data in a distributed system.\n",
    "- `dask.bag`: distributed generic python list. The Dask equivalent to a PySpark RDD\n",
    "- `dask.array`: distributed numpy arrays\n",
    "- `dask.dataframe`: distributed pandas dataframes\n",
    "\n",
    "All the high-level data structure APIs are optimized to exploit the DAG optimization features of the Dask scheduler, and thus rely on lazy computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21d093",
   "metadata": {},
   "source": [
    "## Start the Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b731ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this variable with one of the following values\n",
    "\n",
    "# -> 'local'\n",
    "# -> 'docker_container'\n",
    "# -> 'docker_cluster'\n",
    "\n",
    "CLUSTER_TYPE ='docker_cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CLUSTER_TYPE $CLUSTER_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d851a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg --out script_out\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    echo \"Launching scheduler and worker\"\n",
    "    \n",
    "    HOSTIP=`hostname -I | xargs`\n",
    "    \n",
    "    echo \"dask-scheduler --host $HOSTIP --dashboard-address $HOSTIP:8787\"\n",
    "    \n",
    "    # dask scheduler \n",
    "    dask-scheduler --host $HOSTIP --dashboard-address $HOSTIP:8787 &\n",
    "\n",
    "    # dask worker\n",
    "    dask-worker $HOSTIP:8786 --memory-limit 2GB --nworkers 2 &\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b38330",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_ip = !hostname -I | xargs\n",
    "host_ip = host_ip[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779422b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "if CLUSTER_TYPE == 'local':\n",
    "    \n",
    "    client = Client()\n",
    "\n",
    "elif CLUSTER_TYPE == 'docker_container':\n",
    "    \n",
    "    client = Client('{}:8786'.format(host_ip))\n",
    "    \n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    \n",
    "    # use the provided master\n",
    "    client = Client('dask-scheduler:8786')\n",
    "    \n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08409bcd",
   "metadata": {},
   "source": [
    "## Dask DataFrame\n",
    "\n",
    "The dask.dataframe API implements a blocked parallel DataFrame object that mimics a large subset of the Pandas DataFrame interfaces. \n",
    "\n",
    "One Dask DataFrame is comprised of many in-memory pandas DataFrames or Series hosted in all machines of the cluster.\n",
    "A Dask DataFrame have its table separated (partitioned) along the index **\\*** \n",
    "\n",
    "One operation on a Dask DataFrame triggers many Pandas operations on the constituent Pandas DataFrames in a way that is mindful of potential parallelism and memory constraints.\n",
    "\n",
    "**\\*** _Quick question. What kind of data partitioning are we talking about in this case? Vertical or Horizontal?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd63846",
   "metadata": {},
   "source": [
    "### On the Data Partitioning\n",
    "\n",
    "Partitioning is one of the key aspects of data management and processing in distributed systems (both in Spark, Dask, but also other frameworks as well).\n",
    "\n",
    "Distributing our dataset into a large number of small partitions enables parallel processing in each node.\n",
    "However, we should always remember that every single call the central scheduler performs to access the data on a remote partition (on a worker) requires time, ideally in the order of a few hundreds milliseconds.\n",
    "\n",
    "As the user of the distributed processing system, it is **our** (not Dask's or Spark's) job to decide and optimize the number of partitions.\n",
    "The distributed framework will first operate a choice or a guess, but the optimization of the performances depend on us making the right choice depending on the task.\n",
    "\n",
    "For example, loading up data from a set of files, initially the number of partitions might be the exactly the number of the CSV files from which we are importing data. \n",
    "However, over time, as we reduce or increase the size of our DataFrames by filtering or joining, it may be wise to reconsider how many partitions we need to optimize Dask scheduler overhead. \n",
    "\n",
    "\n",
    "_There is always a cost associated in having too many or having too few partitions, but unfortunately, at the same time we don't have a single rule to decide which is the single \"right\" number of partitions._\n",
    "\n",
    "\n",
    "As a rule of thumb, we can say that partitions should fit comfortably in memory.\n",
    "But at the same time, they must not be too many, such as the central scheduler overhead becomes impacting the performances of our computation.\n",
    "\n",
    "This means that the number of partition should be in a \"reasonable region\", and their number and size must be chosend by the user when starting optimizing the execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab6863",
   "metadata": {},
   "source": [
    "Let's start by reading a set of structured files (comma separated) in a DataFrame.\n",
    "\n",
    "In pure Pandas, we have to loop over all files, open each one of them as a DataFrame, and concatenate all dataframes into a large DF.\n",
    "\n",
    "The resulting DF is a single entity, stored in memory for single-threaded data access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67799e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls datasets/accounts_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -10 datasets/accounts_csv/accounts.0.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff25aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import iglob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = os.path.join('datasets','accounts_csv','accounts.*.csv')\n",
    "\n",
    "all_files = iglob(path, recursive=True)     \n",
    "\n",
    "dataframes = (pd.read_csv(f) for f in all_files)\n",
    "\n",
    "large_dataframe = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46bcad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "large_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8aa2f9",
   "metadata": {},
   "source": [
    "In Dask, DataFrames can be created from a glob pattern using the wildcard `*`, so all files in the path matching that pattern will be read into the same Dask DataFrame.\n",
    "\n",
    "We should remember that Dask DataFrames are a collection of Pandas dataframes, scattered across the workers.\n",
    "\n",
    "When reading data into a DataFrame Dask will automatically create a number of jobs to read out the data in parallel.\n",
    "It does this by creating one parallel job per chunk, which by default is the number of input files.\n",
    "We can (and generally speaking should) however always think about how we want to partition our dataset, to exploits at best our workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e80a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "path = os.path.join('datasets','accounts_csv','accounts.*.csv')\n",
    "\n",
    "df = dd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d77131",
   "metadata": {},
   "source": [
    "As always, reading data is a lazy operation, posponed until we have something to compute.\n",
    "\n",
    "Let's compute the lenght of the total DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7fe7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda64378",
   "metadata": {},
   "source": [
    "Only at this stage, each file was loaded into a Pandas DataFrame and scattered on the nodes.\n",
    "\n",
    "Computing `len()` meant that each Pandas DataFrame had `len` applied to it, and then the sub-result were aggregated and combined to provide the overall total length of the DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713351ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813a31c",
   "metadata": {},
   "source": [
    "We can re-assign the number of partitions to the Dask DataFrame object using the `repartition` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef75a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(npartitions=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b9805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcdda7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5703b7",
   "metadata": {},
   "source": [
    "It is worth mentioning that differently from Pandas, Dask only reads in a sample from the beginning of the file (or the list of files) to start inferring the Data Types.\n",
    "\n",
    "These inferred datatypes are then enforced when reading all partitions, which may lead to *incorrect* datatype assignment!\n",
    "\n",
    "Think for instance in the case we start reading a csv file for which the first $n$ entries of a column are all integers, but the column is actually supposed to be a _string_ type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c46f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join('datasets', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70641981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c37398b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed6418",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bddaa4",
   "metadata": {},
   "source": [
    "In this case, the datatypes inferred for `CRSElapsedTime` and `TailNum` are in fact incorrect. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3872d18",
   "metadata": {},
   "source": [
    "We can however force Dask to interpret the datatypes as we want them to be using the `dtype` assignment.\n",
    "This is usually the most viable and robust solution, but not the only one.\n",
    "\n",
    "We could also:\n",
    "- Increase the size of the sample used to infer the datatypes\n",
    "- Use `assume_missing` to make Dask assume that columns inferred to be `int` (which don't allow missing values) are actually `floats` (which do allow missing values)\n",
    "\n",
    "However, in general the best solution is still to hard-code the datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f5e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join('datasets', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': str,\n",
    "                        'CRSElapsedTime': float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3dca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d8d26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec163a79",
   "metadata": {},
   "source": [
    "While on the topic of reading out data from files, it is worth mentioning that Dask offers a vast range of APIs for importing structured data from a multitude of file formats into a DataFrame object, including:\n",
    "- csv\n",
    "- json\n",
    "- avro\n",
    "- parquet\n",
    "\n",
    "For a more extensive list, check the official documentation at the [link](https://docs.dask.org/en/stable/dataframe-create.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37b7b2f",
   "metadata": {},
   "source": [
    "### Computations with Dask DataFrames\n",
    "\n",
    "Dask DataFrame almost 1-to-1 copy the Pandas DataFrame API.\n",
    "Effectively, Dask provides a wrapper around the Pandas API in order to manage the same calls on the distributed collection of Pandas DataFrames, which in our case are the partitions or our dataset.\n",
    "\n",
    "Clearly, using Dask DataFrame is useful only up to the measure where Pandas shows its limits, either due to data size (larger than memory datasets) or computation speed (running a task that could be parallelized efficiently).\n",
    "\n",
    "Whenever working with small datasets and not-really parallelizable tasks, Pandas will always be the best choice.\n",
    "\n",
    "For this very reason, the usual computing pattern when dealing with Dask dataframe is simply the following:\n",
    "1. Use `Dask Bag` to ingest data from semi/unstructured sources and pre-process it into Dask DataFrames\n",
    "1. Use `Dask DataFrame` for parallel computations and data reduction to produce a resulting slimmed DataFrame that can be fit in memory, then convert it into a single Pandas DataFrame\n",
    "1. Use `Pandas` for simple (yet fast) DataFrames operations for non-parallelizable tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ba854",
   "metadata": {},
   "source": [
    "We can always retrieve data from all partitions and create a Pandas DataFrame from a Dask DataFrame using the `compute` method on the DataFrame itself.\n",
    "\n",
    "At this point this should not come as a surprise, but this operation should be done very carefully, and only at the right time, when we have reduced our very large (possibly TeraBytes-large) Dask DataFrame to something meaningful to be stored in our computers' memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe13aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.compute()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900aa585",
   "metadata": {},
   "source": [
    "Let's work with basic DataFrame operations and inspect their impact in Dask vs Pandas.\n",
    "\n",
    "In Dask, all operations on columns and individual items are easily parallelized, and thus very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the max of the DepDelay column\n",
    "df.DepDelay.max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30accc15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize the graph for this operation\n",
    "df.DepDelay.max().visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the average arrival delay (ArrDelay) of all flights with carrier UnitedAirlines (UA)\n",
    "df[df.UniqueCarrier=='UA'].ArrDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbd90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the last operation\n",
    "df[df.UniqueCarrier=='UA'].ArrDelay.mean().visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c727a39",
   "metadata": {},
   "source": [
    "Dask DataFrames also offer a smart implementation of most operations involving a minimal amount of shuffling, such as group based aggregations and merge operations.\n",
    "\n",
    "This means that operations such as `groupby`, `resample`, `rolling` and other similar operations are still reasonably fast, even though they are produced with a large under-the-hood optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d05a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the max airtime by combinations of origin and destination\n",
    "df.groupby(['Origin','Dest']).AirTime.max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the last operation\n",
    "df.groupby(['Origin','Dest']).AirTime.max().visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506bb15e",
   "metadata": {},
   "source": [
    "As we can see from the graph, the `groupby` method returns a single object from the computation, stored in a single partition.\n",
    "This is usually a fair choice, as it is normally considered that the result of a groupby aggregation is small enough to be stored into a single worker memory, thus not requiring the result to be split into multiple partitions.\n",
    "\n",
    "\n",
    "However, it may happen that the retuned object is very large, depending on the input datasets, and we can optimize the number of output partitions by using the `split_out` argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c55d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the max airtime by combinations of origin and destination\n",
    "# split the result of the groupby operation in 4 partitions\n",
    "df.groupby(['Origin','Dest']).AirTime.max(split_out=4).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6b263",
   "metadata": {},
   "source": [
    "Note: Dask also supports the same aggregate syntax as Pandas to run sevearal aggregations at the same time on the same group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a5b200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['Origin','Dest']).AirTime.aggregate(['mean','std']).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a0bacc",
   "metadata": {},
   "source": [
    "There are clearly operations for which the mapping with the standard Pandas APIs cannot hold.\n",
    "\n",
    "For instance, the slicing and feature-based indexing based on `loc` does work as expected, but the potition-based indexing operator `iloc` does not really work as one would expect in Dask vs Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Dest']=='DEN',['UniqueCarrier']].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91016aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Dest']=='DEN',['UniqueCarrier']].visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae38193",
   "metadata": {},
   "source": [
    "`iloc` will raise instead an exception.\n",
    "\n",
    "This happens because Dask DataFrame does not take into account the length of partitions and the row ordering inside the partition.\n",
    "After all, what is the real odering of rows when the records are sharded across multiple nodes...? \n",
    "\n",
    "We can still use `iloc` to select the index of a some column, but for all rows in all partitions, e.g.: `iloc[:,0:10]`\n",
    "\n",
    "This makes `iloc` extremely inefficient in Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d3ad8f",
   "metadata": {},
   "source": [
    "Other very ineffienct operations in Dask are all the actions that require a re-indexing of the entire DataFrame, or sorting or merging of the entries based on a column that is not the index of the DataFrame itself.\n",
    "\n",
    "This is due to the optimization Dask runs under the hood, where the range of the index column field can (although this might not always be the case) be used to subdivide the data into partitions.\n",
    "\n",
    "Setting an index on a DataFrame will thus require to sort the entire dataset by the specified column, which is an extremely expensive process.\n",
    "While the sorting process can be very slow, it can be extremely usefult to do it (although _very_ unfrequently) to speed up further computations.\n",
    "\n",
    "Dask will in fact also use the index in merge/join operations. Performing a join on a column that is not the index of the DataFrame is also a very expensive operations.\n",
    "\n",
    "After an unavoidable and expensive operation such a full reshuffling of your data is done (out of necessity), one can always persist the new DataFrame, to speed up subsequent computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e7189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_reindexed = df[(df.UniqueCarrier=='UA') & (df['Dest']=='DEN')].set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b95f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_reindexed.ArrDelay.rolling('5D').mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f34bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_reindexed.ArrDelay.rolling('5D').mean().visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350421a",
   "metadata": {},
   "source": [
    "### Shared, reapeated and intermediate computations\n",
    "\n",
    "When computing all of the above, we sometimes have to repeat the same operation more than once. \n",
    "\n",
    "For most operations, Dask DataFrame _hashes_ the arguments allowing duplicate computations to be shared, and only computed once.\n",
    "\n",
    "For example, lets compute the mean and standard deviation for departure delay of all non-canceled flights. \n",
    "Since dask operations are lazy, those values aren't the final results yet. \n",
    "They're just the recipe require to get the result.\n",
    "\n",
    "If we want to change the approach, we can compute them with two individual calls to compute. \n",
    "In this latter case, there is no sharing of intermediate computations and an overall speedup of the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd69d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7675705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "#create non-cancelled dataframe\n",
    "non_cancelled = df[df.Cancelled==0]\n",
    "\n",
    "# create mean DepDelay series\n",
    "mean_delay    = non_cancelled.DepDelay.mean()\n",
    "\n",
    "# create std DepDelay series\n",
    "std_delay     = non_cancelled.DepDelay.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18710159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# compute the time to run both functions\n",
    "mean_delay_res = mean_delay.compute()\n",
    "std_delay_res = std_delay.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a2fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# compute the time to combining both functions into a single compute call\n",
    "mean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7035b20",
   "metadata": {},
   "source": [
    "Using dask.compute takes ~ 1/2 of the time of the other call. \n",
    "\n",
    "This is because the task graphs for both results are merged when calling `dask.compute`, allowing shared operations to only be done once instead of twice. \n",
    "In particular, using dask.compute only does the following once:\n",
    "\n",
    "+ the calls to the `read_csv` function\n",
    "+ the filter on the cancelled flights\n",
    "+ some of the necessary reductions (sum, count)\n",
    "\n",
    "Let's see the computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99272d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247468d7",
   "metadata": {},
   "source": [
    "What we can see is that a lot of operation have been merged and used one single time, thus resulting in a optimization of the computing time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99b0ee",
   "metadata": {},
   "source": [
    "## Re-run the same example from the Spark notebook using Dask\n",
    "\n",
    "To apply all we discussed about Dask high- and low-level APIs, we can re-run the same task we used as an example analysis pipeline in pySpark for the DiMuon invariant mass of the LHC collision data.\n",
    "\n",
    "Starting from the JSON files already used in the previous pySpark example we will need to:\n",
    "1. load all files using the Dask Bag API\n",
    "2. parse the json event collections into python dictionaries\n",
    "3. use a foldby to count the number of events per sample (mc and data)\n",
    "4. plot the distribution of the number of muons in the mc and data sample from the Dask.Bag object\n",
    "5. select only events with exactly 2 muons with opposite charge and fill a Dask.DataFrame with the normalized results\n",
    "6. create the tranverse momemta $p_{T,1}$ and $p_{T,2}$ features and create a 2d plot of these features\n",
    "$$p_T = \\sqrt{p_x^2 + p_y^2}$$\n",
    "7. retain only the dimuon pairs where both muons' $p_T$ is greater than 15 (GeV)\n",
    "8. create the components of the dimuon 4-momentum\n",
    "$$(E,P_x,P_y,P_z) = (E_1+E_2,p_{x,1}+p_{x,2},p_{y,1}+p_{y,2},p_{z,1}+p_{z,2})$$\n",
    "9. create and plot the invariant mass spectrum\n",
    "$$M = \\sqrt{E^2 - (P_x^2 + P_y^2 + P_z^2) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81fd024",
   "metadata": {},
   "source": [
    "Before starting, from a terminal (external to the docker container you are working in), copy the `MAPD-B/spark/datasets/lecture2/dimuon` folder into the `MAPD-B/dask/notebooks/datasets/.` path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37cfdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1870e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the dimuon json files in a bag, each containing the text lines\n",
    "import dask.bag as db\n",
    "from pprint import pprint\n",
    "\n",
    "db_lines = db.read_text(os.path.join('datasets','dimuon','*.json'),\n",
    "                 files_per_partition=4)\n",
    "pprint(db_lines.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02f508",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# extract the json structure from the lines\n",
    "import json\n",
    "db_js = db_lines.map(json.loads).flatten()\n",
    "pprint(db_js.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2fad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of events per each sample\n",
    "from operator import add\n",
    "\n",
    "def incr(tot, _):\n",
    "    return tot+1\n",
    "\n",
    "db_js.foldby(key='sample', \n",
    "             binop=incr, \n",
    "             initial=0, \n",
    "             combine=add, \n",
    "             combine_initial=0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the number of muons in the mc and data sample\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "mc_counts = db_js.filter(lambda record: record['sample']=='mc')\\\n",
    "                     .pluck('nMuon').compute()\n",
    "plt.hist(\n",
    "    mc_counts,\n",
    "    label = 'MC simulation', bins=range(16), alpha=0.6\n",
    ")\n",
    "\n",
    "data_counts = db_js.filter(lambda record: record['sample']=='data')\\\n",
    "                     .pluck('nMuon').compute()\n",
    "data_counts, data_edges = np.histogram(data_counts,range(16))\n",
    "data_centers = 0.5*(data_edges[1:] + data_edges[:-1]) \n",
    "\n",
    "plt.errorbar(\n",
    "    data_centers,\n",
    "    data_counts,\n",
    "    yerr = np.sqrt(data_counts),\n",
    "    label = 'Data', color='black', fmt='o'\n",
    ")\n",
    "\n",
    "plt.xlim(-0.5,16.5)\n",
    "plt.xlabel(\"Number of muons\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03589adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only events with exactly 2 muons with opposite charge and fill a dask dataframe with the results\n",
    "def flatten_records(record):\n",
    "    return {\n",
    "        'sample': record['sample'],\n",
    "        'E1':  record['Muons'][0]['E'],\n",
    "        'px1': record['Muons'][0]['px'],\n",
    "        'py1': record['Muons'][0]['py'],\n",
    "        'pz1': record['Muons'][0]['pz'],\n",
    "        'E2':  record['Muons'][1]['E'],\n",
    "        'px2': record['Muons'][1]['px'],\n",
    "        'py2': record['Muons'][1]['py'],\n",
    "        'pz2': record['Muons'][1]['pz'],\n",
    "    }\n",
    "\n",
    "df = db_js.filter(lambda record: record['nMuon']==2)\\\n",
    "          .filter(lambda record: record['Muons'][0]['charge']==-record['Muons'][1]['charge'])\\\n",
    "          .map(flatten_records)\\\n",
    "          .to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b476bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the new pT1 and pT2 features \n",
    "# pT = sqrt(px^2 + py^2)\n",
    "df['pT1'] = np.sqrt(df['px1']**2 + df['py1']**2) \n",
    "df['pT2'] = np.sqrt(df['px2']**2 + df['py2']**2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 2 dimensional distribution of the dimuon pair pTs in the mc sample\n",
    "import matplotlib.colors as clt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.hist2d(\n",
    "    df.loc[:,'pT1'],\n",
    "    df.loc[:,'pT2'],\n",
    "    label = 'MC simulation', \n",
    "    bins=(range(0,100,2),range(0,100,2)), \n",
    "    norm=clt.LogNorm()\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel(\"$p_{T,1}$ (Gev)\")\n",
    "plt.ylabel(\"$p_{T,2}$ (Gev)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only those dimuon pairs where both muons' pT is greater than 15 \n",
    "df = df.loc[(df['pT1']>15) & (df['pT2']>15),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fdb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 4-momentum features \n",
    "df['E']  = df['E1']  + df['E2']\n",
    "df['Px'] = df['px1'] + df['px2']\n",
    "df['Py'] = df['py1'] + df['py2']\n",
    "df['Pz'] = df['pz1'] + df['pz2']\n",
    "\n",
    "# and compute the invariant mass of the dimuon pair\n",
    "# M = sqrt(E*E - (Px*Px + Py*Py + Pz*Pz))\n",
    "df['M'] = np.sqrt(df['E']**2 - (df['Px']**2 + df['Py']**2 + df['Pz']**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the slimmed down dataframe containing only the sample, Mass, and pT of the objects\n",
    "# and store it in a Pandas DataFrame\n",
    "pandas_df = df.loc[:,['sample','M','pT1','pT2']].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da766d65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the distribution of the dimuon invariant mass in the mc and data sample\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.hist(\n",
    "    pandas_df.loc[pandas_df['sample']=='mc','M'],\n",
    "    label = 'MC simulation', bins=range(120), alpha=0.6\n",
    ")\n",
    "\n",
    "data_counts, data_edges = np.histogram(pandas_df.loc[pandas_df['sample']=='data','M'],range(120))\n",
    "data_centers = 0.5*(data_edges[1:] + data_edges[:-1]) \n",
    "\n",
    "plt.errorbar(\n",
    "    data_centers,\n",
    "    data_counts,\n",
    "    yerr = np.sqrt(data_counts),\n",
    "    label = 'Data', color='black', fmt='.'\n",
    ")\n",
    "\n",
    "plt.xlim(-0.5,120.5)\n",
    "plt.xlabel(\"Number of muons\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67a7907",
   "metadata": {},
   "source": [
    "### Memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reclaim some memory from the workers\n",
    "import ctypes\n",
    "\n",
    "def trim_memory() -> int:\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    return libc.malloc_trim(0)\n",
    "\n",
    "client.run(trim_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d376528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to restart the client\n",
    "client.restart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
