{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab300ecc",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "<center style =\"font-size: xx-large; font-weight: 600; line-height: 1.1;\">\n",
    "Lecture 2: Class Notes</center>  \n",
    "\n",
    "## Contents \n",
    "  \n",
    "* [Setup: Docker Cluster](#cluster)  \n",
    "* [Spark DataFrame](#c1)\n",
    "* [pyspark.pandas API](#c2)  \n",
    "* [Dimuon Example](#muon)  \n",
    "* [SQL like-view in spark, sql queries ect ](#sql)\n",
    "* [Pandas UDFs](#udf)\n",
    "* [(Re-)Discovering particles](#disco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91590d5d",
   "metadata": {},
   "source": [
    "# Lecture 2: Spark Dataframes\n",
    "\n",
    "Spark SQL is the Spark module dedicated to store and process structured datasets, e.g organized in columns such as Pandas Dataframes or Relational Databases tables. \n",
    "\n",
    "In Spark, structured datasets are referred to as Spark Dataframes, which anyway rely on the RDD way of storing and partitioning the data, but also provides richer optimizations under the hood.\n",
    "\n",
    "As for the previous lecture, select the most appropriate variable based on where this notebook is run. \n",
    "If the docker cluster is used, the process of starting spark cluster can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e8b5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this variable with one of the following values\n",
    "\n",
    "# -> 'local'\n",
    "# -> 'docker_container'\n",
    "# -> 'docker_cluster'\n",
    "\n",
    "CLUSTER_TYPE ='docker_cluster'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38914c8b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Start the cluster \n",
    "\n",
    "Environment variables need to be set only in the case of a local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f4d348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLUSTER_TYPE=docker_cluster\n"
     ]
    }
   ],
   "source": [
    "# set an enviromental variable\n",
    "\n",
    "%env CLUSTER_TYPE $CLUSTER_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e05f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLUSTER_TYPE=='local':\n",
    "    import findspark\n",
    "    findspark.init('/home/pazzini/work/courses/MAPD_B/MAPD-B/spark/spark-3.2.1-bin-hadoop3.2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca6be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    echo \"Launching master and worker\"\n",
    "    \n",
    "    # start master \n",
    "    $SPARK_HOME/sbin/start-master.sh --host localhost \\\n",
    "        --port 7077 --webui-port 8080\n",
    "    \n",
    "    # start worker\n",
    "    $SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 \\\n",
    "        --cores 2 --memory 2g\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9907c1",
   "metadata": {},
   "source": [
    "## Create the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ee5986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/bin/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/11 07:07:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d80d81727c31:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>First spark application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f48d0185760>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://localhost:7077\")\\\n",
    "        .appName(\"First spark application\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    \n",
    "    # use the provided master\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .appName(\"First spark application\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5beebfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d80d81727c31:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>First spark application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-master:7077 appName=First spark application>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0dbef",
   "metadata": {},
   "source": [
    "<a href =\"#contents\">\n",
    "<p style=\"text-align: right;\">return to contents</p>\n",
    "</a>  \n",
    "<a id=\"c1\"></a>  \n",
    "\n",
    "## Creating a simple DataFrame\n",
    "\n",
    "Spark DataFrame is already very similar to the Pandas DataFrame API, and it can be created in a multitude of possible ways:\n",
    "\n",
    "- creating and appending a list of Rows (Spark objects for records) with an implicit schema definition\n",
    "- creating and appending a list of Python tuples with a explicit schema definition\n",
    "- importing a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3f820ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date, datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c118bbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6383baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a4973",
   "metadata": {},
   "source": [
    "As in the case of the RDD, the mere creation of the DataFrame does not imply any work is submitted to the executors.\n",
    "Whenever we ask Spark to retrieve something we instead trigger the execution of the jobs (as usual, subdivided in stages and tasks).\n",
    "\n",
    "To visualize the content of a Spark DataFrame we have similar interfaces to Pandas:\n",
    "- `DataFrame.show()` to visualize its content\n",
    "- `DataFrame.printSchema()` to visualize the schema of the structured dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "099b1ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "016c8dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c8db3",
   "metadata": {},
   "source": [
    "## Loading structured data\n",
    "\n",
    "By default Spark can create a DataFrame from data stored in many formats such as `csv`, `json` and many other listed [here](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html). \n",
    "\n",
    "If your dataset is stored in a format that Spark cannot understand natively, it is always possible to first create an RDD, and later convert the RDD into a DataFrame with the `toDF()` functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba4a26",
   "metadata": {},
   "source": [
    "Here we create some fake structured data in the form of a simple set of records with 2 features: `feature1` and `feature2`.\n",
    "\n",
    "We can parallelize the creation of the Rows in Spark as if we were loading and \"unpacking\" data from a number of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "340d522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Row is the equivalent of a record\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def read_custom_data(file_name):\n",
    "    # pretend to load some real data\n",
    "    #   this is some FAKE placeholder code, a placeholder for some \n",
    "    #   operation you might have to perform when reading the files\n",
    "    custom_data = []\n",
    "    for i in range(5):\n",
    "        event = {\n",
    "            'feature1': np.random.random(),\n",
    "            'feature2': np.random.random()\n",
    "        }\n",
    "        custom_data.append(Row(**event))\n",
    "    return custom_data\n",
    "    \n",
    "# read some files in parallel and create a dataframe\n",
    "file_list = ['file1', 'file2']\n",
    "\n",
    "rdd = sc.parallelize(file_list)\\\n",
    "        .flatMap(read_custom_data)\n",
    "\n",
    "df = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7076a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(feature1=0.3427651151118485, feature2=0.06913107659610762),\n",
       " Row(feature1=0.4940726923836122, feature2=0.16975532542762672),\n",
       " Row(feature1=0.2730250334300308, feature2=0.6326196670436508),\n",
       " Row(feature1=0.7238833898249569, feature2=0.9092510460807073),\n",
       " Row(feature1=0.9423875281328208, feature2=0.006365632784912867),\n",
       " Row(feature1=0.3893134139790455, feature2=0.9505897693600379),\n",
       " Row(feature1=0.8316120203057057, feature2=0.682327267953622),\n",
       " Row(feature1=0.17374233666777483, feature2=0.10079336398867422),\n",
       " Row(feature1=0.40455800605707004, feature2=0.913054806066879),\n",
       " Row(feature1=0.2827223386754386, feature2=0.7840844754495215)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "def7128a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(feature1=0.32428025675405203, feature2=0.15411876812592773),\n",
       " Row(feature1=0.03851173055263557, feature2=0.6507641014670522),\n",
       " Row(feature1=0.8061976233967022, feature2=0.9844655823174588),\n",
       " Row(feature1=0.5865531796644876, feature2=0.028799065205216023),\n",
       " Row(feature1=0.6256243179600866, feature2=0.29017393735670116),\n",
       " Row(feature1=0.9553234046627685, feature2=0.7010730135887695),\n",
       " Row(feature1=0.19277718855012882, feature2=0.943494147287571),\n",
       " Row(feature1=0.21382957766696054, feature2=0.397092640165427),\n",
       " Row(feature1=0.1584512333616246, feature2=0.08921271206095494),\n",
       " Row(feature1=0.7059980723578853, feature2=0.13092377207328543)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a collect() will always apply on the low-level Spark API, the RDD\n",
    "df.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db0a3fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           feature1|           feature2|\n",
      "+-------------------+-------------------+\n",
      "| 0.8420484450493786| 0.3699271155773296|\n",
      "| 0.8644908836735762| 0.2692338899512664|\n",
      "| 0.8588912238343377|0.28474872810460916|\n",
      "|0.21253480470700314| 0.6105576763287208|\n",
      "| 0.6668161564648584| 0.9958161755597967|\n",
      "+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a show() will instead \"wrap\" aroud the RDD to present you the output with the proper DataFrame schema\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ffbf4c",
   "metadata": {},
   "source": [
    "<a href =\"#contents\">\n",
    "<p style=\"text-align: right;\">return to contents</p>\n",
    "</a>  \n",
    "<a id=\"c2\"></a>  \n",
    "\n",
    "\n",
    "## The `pyspark.pandas` API\n",
    "\n",
    "Very recently PySpark introduced a dedicated `Pandas` API on Spark, geared mainly for new users to make it even easier to move to and from the Pandas DataFrame and the PySpark DataFrame.\n",
    "\n",
    "We will mostly use the PySpark DataFrame in this notebook. You are suggested to take a look at the documentation on the Pandas API on Spark at the [link](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "066bf1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c4f88",
   "metadata": {},
   "source": [
    "With the pyspark.pandas API the DataFrame creation is now streamlined and almost identical to plain Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fad249e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "045129e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>six</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a    b      c\n",
       "10  1  100    one\n",
       "20  2  200    two\n",
       "30  3  300  three\n",
       "40  4  400   four\n",
       "50  5  500   five\n",
       "60  6  600    six"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26bfb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_pandas_df = ps.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "702518fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>six</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a    b      c\n",
       "10  1  100    one\n",
       "20  2  200    two\n",
       "30  3  300  three\n",
       "40  4  400   four\n",
       "50  5  500   five\n",
       "60  6  600    six"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark_pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c716ba",
   "metadata": {},
   "source": [
    "Most of the common `Pandas` functions will still work on `pyspark.pandas`, so most probably a good fraction of the pure-Pandas code you have might as well work in Spark with little effort. \n",
    "\n",
    "**BUT**\n",
    "\n",
    "One also have to keep in mind that ***Pandas and PySpark are very different \"under the hood\"***.\n",
    "Where Pandas is single-thread and will host the DataFrame data in memory, PySpark is designed to host data into partitions using the executors' memory (or storage) and process it in a distributed way.\n",
    "\n",
    "This imply that many operations that are ideally very simple might be either impossible or simply very inefficient in PySpark.\n",
    "\n",
    "_For instance... how do you think the ordering of the entries in the table is handled when your dataframe is split into possibly hundreds of partitions?\n",
    "What if you want a dataframe to be sorted?_\n",
    "\n",
    "The data in a Spark dataframe does not preserve the natural order by default. \n",
    "We can force Spark to preserve the natural order but this causes a performance overhead with an additional stage of internal data sorting (a wide-depenency transformation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2f9e1",
   "metadata": {},
   "source": [
    "<a href =\"#contents\">\n",
    "<p style=\"text-align: right;\">return to contents</p>\n",
    "</a>  \n",
    "<a id=\"muon\"></a>  \n",
    "\n",
    "## Dimuon mass Example\n",
    "\n",
    "Several particles decay in a pair of opposite charged leptons (electrons, muons and taus).\n",
    "\n",
    "The dimuon spectrum, computed by calculating the invariant mass of muon pairs with opposite charge, features the presence of a number of narrow resonances, corresponding to the mass of the parent particle: from the η meson at about 548 MeV  up to the Z boson at about 91 GeV.\n",
    "\n",
    "Rare processes are also associated to this very same final state, such as the Bs dimuon decay (first observed in 2012 at CMS and LHCb), and the elusive Higgs dimuon decay (for which there is statistical evidence, but not yet an observation); new yet undiscovered particles might also show up as new resonances in the dimuon spectrum as the statistics and accelerator energy is increased.\n",
    "\n",
    "![Event Display](imgs/lecture2/event_display.png)\n",
    "\n",
    "The dataset used in this exercise is taken from the CERN Open Data portal (https://opendata.cern.ch/) and represents a portion of the data collected by the CMS collaboration at the Large Hadron Collider in 2010.\n",
    "\n",
    "This dataset comprise of only the fraction of events retained by online selections (trigger) which identify collisions where muons have been produced, thus storing only about 10 events out of the about 40 millions of collisions produced every second at LHC.\n",
    "\n",
    "The whole dataset collected by the CMS collaboration since the start of LHC operations in 2010 is comprised of tens of PBs of data and simulations.\n",
    "\n",
    "\n",
    "A subset of events have been extracted and preprocessed, the resulting files being stored as JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b64245b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load dataset on dataset/lecture2/dimuon\n",
    "#    spark.read --> read from a source of data\n",
    "#    format     --> choose between the available (semi)structured data formats\n",
    "#    load       --> the path to the input files\n",
    "df = spark.read \\\n",
    "    .option(\"inferTimestamp\",\"false\") \\\n",
    "    .option(\"prefersDecimal\",\"false\") \\\n",
    "    .format('json') \\\n",
    "    .load('../datasets/lecture2/dimuon/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d189a6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Muons: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- E: double (nullable = true)\n",
      " |    |    |-- charge: long (nullable = true)\n",
      " |    |    |-- px: double (nullable = true)\n",
      " |    |    |-- py: double (nullable = true)\n",
      " |    |    |-- pz: double (nullable = true)\n",
      " |-- nMuon: long (nullable = true)\n",
      " |-- sample: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the dataset schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968c579",
   "metadata": {},
   "source": [
    "This shows that the data is not \"flat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e41b580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|               Muons|nMuon|sample|\n",
      "+--------------------+-----+------+\n",
      "|[{17.492634165117...|    2|  data|\n",
      "|[{11.518019014184...|    2|  data|\n",
      "|[{15.121118467960...|    1|  data|\n",
      "|[{29.141155937086...|    4|  data|\n",
      "|[{14.601779359675...|    4|  data|\n",
      "+--------------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the first few (e.g. 5) rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddf7c767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|sample|count|\n",
      "+------+-----+\n",
      "|    mc|25000|\n",
      "|  data|25000|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count the number of events per each sample\n",
    "# \n",
    "#   1. --> apply a .groupBy() transformation on the dataframe, grouping on the sample\n",
    "#   2. --> use a .count() aggregator \n",
    "#   3. --> show the results\n",
    "\n",
    "df.groupby('sample').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "473375c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist the dataframe into memory to avoid\n",
    "# loading it every time we want to use it\n",
    "\n",
    "#perssit does nothing until needed\n",
    "df=df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08b5e7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56cdbaad",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2578079437.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [26]\u001b[0;36m\u001b[0m\n\u001b[0;31m    num_muons_dist =\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# get the number of muons\n",
    "# distribution and plot it\n",
    "# \n",
    "#   1. --> apply a .groupBy() transformation on the dataframe, grouping on the sample, and on the number of muons\n",
    "#   2. --> use a .count() aggregator \n",
    "#   3. --> sort by the number of muons\n",
    "#   4. --> collect the results\n",
    "\n",
    "num_muons_dist = df.groupBy('sample', 'nMuon')\\\n",
    "    .count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f88c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the first few entries \n",
    "num_muons_dist[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0228a7a7",
   "metadata": {},
   "source": [
    "Plotting this data from Spark alone would require a lot of code, but fortunately there is a trick we can use: it is possible to convert a Spark dataframe into a Pandas Dataframe with `toPandas()`. \n",
    "\n",
    "Again, as it happens for `collect()`, the entire resulting dataset is fetched into the master and it may not fit in the memory, so do not use `toPandas()` very lightly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24545648",
   "metadata": {},
   "source": [
    "(**Side Note on Apache Arrow**)\n",
    "_Apache Arrow comes into play in this contex: it is an in-memory columnar data format that is used in spark to efficiently transfer data between the JVM and Python processes. When it is not enabled Spark comunicates with python processes by serializing and deserializing one element of the time. With Arrow this operation is \"vectorized\", i.e. one column at the time is transfered. This is possible because both spark and pandas included Arrow representation._\n",
    "\n",
    "_[Here](https://xuechendi.github.io/2019/04/16/Apache-Arrow) you can find a nice blog post explaining how it works._\n",
    "\n",
    "_In this way, operations between PySpark and Pandas are more efficient and we can try to get the best from both worlds._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ee0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# run the same dataframe operation as before, \n",
    "# but instead of collecting the results, \n",
    "# put the results into a pandas dataframe using the .toPandas() function\n",
    "num_muons_dist = \n",
    "\n",
    "num_muons_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569dada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the type of this object \n",
    "# to be sure this is now a real pandas DataFrame\n",
    "type(num_muons_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac386f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the histogram\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "mc_counts = num_muons_dist.loc[num_muons_dist['sample']=='mc']\n",
    "plt.bar(\n",
    "    mc_counts['nMuon'], mc_counts['count'],\n",
    "    label = 'Simulation', width=1, alpha=0.6\n",
    ")\n",
    "\n",
    "data_counts = num_muons_dist.loc[num_muons_dist['sample']=='data']\n",
    "plt.errorbar(\n",
    "    data_counts['nMuon'],data_counts['count'],\n",
    "    yerr = np.sqrt(data_counts['count']),\n",
    "    label = 'Data', color='black', fmt='o'\n",
    ")\n",
    "plt.xlim(-0.5,16.5)\n",
    "plt.xlabel(\"Number of muons\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdc9ac",
   "metadata": {},
   "source": [
    "We are interested in dimuon events, i.e. events having 2 muons. \n",
    "\n",
    "We can do this with:\n",
    "- \"plain dataframe\"-like selections, e.g. `df[df.column > xyz]`\n",
    "- `filter` \n",
    "- `where`\n",
    "- SQL-like statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only events with two muons in a \"plain dataframe\" style\n",
    "df[df['nMuon']==2].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aad103e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|               Muons|nMuon|sample|\n",
      "+--------------------+-----+------+\n",
      "|[{17.492634165117...|    2|  data|\n",
      "|[{11.518019014184...|    2|  data|\n",
      "|[{65.955784364436...|    2|  data|\n",
      "|[{14.860828916586...|    2|  data|\n",
      "|[{11.205213649577...|    2|  data|\n",
      "+--------------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# use a where statement on the column objects from spark\n",
    "df.where(col('nMuon')==2).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d815fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|               Muons|nMuon|sample|\n",
      "+--------------------+-----+------+\n",
      "|[{17.492634165117...|    2|  data|\n",
      "|[{11.518019014184...|    2|  data|\n",
      "|[{65.955784364436...|    2|  data|\n",
      "|[{14.860828916586...|    2|  data|\n",
      "|[{11.205213649577...|    2|  data|\n",
      "+--------------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a where statement on the content of the dataframe\n",
    "dimuon_df = df.where(df['nMuon']==2)\n",
    "dimuon_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "847e1641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|               Muons|nMuon|sample|\n",
      "+--------------------+-----+------+\n",
      "|[{17.492634165117...|    2|  data|\n",
      "|[{11.518019014184...|    2|  data|\n",
      "|[{65.955784364436...|    2|  data|\n",
      "|[{14.860828916586...|    2|  data|\n",
      "|[{11.205213649577...|    2|  data|\n",
      "+--------------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a filter statement on the content of the dataframe\n",
    "df.filter(df['nMuon']==2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc0935",
   "metadata": {},
   "source": [
    "<a href =\"#contents\">\n",
    "<p style=\"text-align: right;\">return to contents</p>\n",
    "</a>  \n",
    "<a id=\"sql\"></a>   \n",
    "\n",
    "### SQL like-view in spark, sql queries ect \n",
    "\n",
    "SQL queries easily optimised by spark   \n",
    "(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b66049d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|               Muons|nMuon|sample|\n",
      "+--------------------+-----+------+\n",
      "|[{17.492634165117...|    2|  data|\n",
      "|[{11.518019014184...|    2|  data|\n",
      "|[{65.955784364436...|    2|  data|\n",
      "|[{14.860828916586...|    2|  data|\n",
      "|[{11.205213649577...|    2|  data|\n",
      "+--------------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a SQL-like view (a \"table\") of the records (POWERFULL COMMONLY USED)\n",
    "df.createOrReplaceTempView(\"dimuon_df_table\")\n",
    "\n",
    "# use plain SQL to filter the events with a SELECT statement\n",
    "sqlDF = spark.sql(\"SELECT * FROM dimuon_df_table WHERE nMuon = 2\")\n",
    "sqlDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c4009",
   "metadata": {},
   "source": [
    "At this stage the Dataset is not flat (we have nested content in the Muons column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c5a9f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Muons: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- E: double (nullable = true)\n",
      " |    |    |-- charge: long (nullable = true)\n",
      " |    |    |-- px: double (nullable = true)\n",
      " |    |    |-- py: double (nullable = true)\n",
      " |    |    |-- pz: double (nullable = true)\n",
      " |-- nMuon: long (nullable = true)\n",
      " |-- sample: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset is not flat\n",
    "dimuon_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a9cf9",
   "metadata": {},
   "source": [
    "Now we can make our dataset flat for further processing: in this way we will be able to perform operations between the columns.\n",
    "\n",
    "A new column can be created by using `withColumn(name, func)` where `func` is a function taking as input one or more columns. An arbitrary complex function can be used to produce the new column: for example we can define a **User Defined Function (UDF)** which will be applied to every row of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c202548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sample: string (nullable = true)\n",
      " |-- nMuon: long (nullable = true)\n",
      " |-- E1: double (nullable = true)\n",
      " |-- px1: double (nullable = true)\n",
      " |-- py1: double (nullable = true)\n",
      " |-- pz1: double (nullable = true)\n",
      " |-- c1: long (nullable = true)\n",
      " |-- E2: double (nullable = true)\n",
      " |-- px2: double (nullable = true)\n",
      " |-- py2: double (nullable = true)\n",
      " |-- pz2: double (nullable = true)\n",
      " |-- c2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create columns for the two muons\n",
    "\n",
    "# select()     projects a set of expressions and returns a new DataFrame\n",
    "# withColumn() creates a new table attribute from the given column\n",
    "# drop()       removes columns from the table\n",
    "dimuon_flat = (\n",
    "    # keep only sample/nMuon/Muons[0]/Muons[1] from the table\n",
    "    dimuon_df.select([col('sample'), col('nMuon'), col('Muons')[0], col('Muons')[1]]) #* still structured objects here\n",
    "    # create a number of additional columns from the Muons[0/1] nested attributes\n",
    "    .withColumn('E1', col('Muons[0].E'))\n",
    "    .withColumn('px1', col('Muons[0].px'))\n",
    "    .withColumn('py1', col('Muons[0].py'))\n",
    "    .withColumn('pz1', col('Muons[0].pz'))\n",
    "    .withColumn('c1',  col('Muons[0].charge'))\n",
    "    .withColumn('E2',  col('Muons[1].E'))\n",
    "    .withColumn('px2', col('Muons[1].px'))\n",
    "    .withColumn('py2', col('Muons[1].py'))\n",
    "    .withColumn('pz2', col('Muons[1].pz'))\n",
    "    .withColumn('c2',  col('Muons[1].charge'))\n",
    "    # remove the original Muons[0] and Muons[1] nested attributes\n",
    "    .drop('Muons[0]', 'Muons[1]')\n",
    ")\n",
    "\n",
    "dimuon_flat.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f5b04",
   "metadata": {},
   "source": [
    "We can now continue with our work and select only the events where the two muons have opposite charge (column `c1` and `c2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "647c018e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'c1' given input columns: [dimuon_df_table.Muons, dimuon_df_table.nMuon, dimuon_df_table.sample]; line 1 pos 50;\n'Project [*]\n+- 'Filter ((nMuon#8L = cast(2 as bigint)) AND ('c1 = -'c2))\n   +- SubqueryAlias dimuon_df_table\n      +- View (`dimuon_df_table`, [Muons#7,nMuon#8L,sample#9])\n         +- Relation [Muons#7,nMuon#8L,sample#9] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# select candidates with opposite charge muons\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m d_flat \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM dimuon_df_table WHERE nMuon = 2 AND c1 == -c2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'c1' given input columns: [dimuon_df_table.Muons, dimuon_df_table.nMuon, dimuon_df_table.sample]; line 1 pos 50;\n'Project [*]\n+- 'Filter ((nMuon#8L = cast(2 as bigint)) AND ('c1 = -'c2))\n   +- SubqueryAlias dimuon_df_table\n      +- View (`dimuon_df_table`, [Muons#7,nMuon#8L,sample#9])\n         +- Relation [Muons#7,nMuon#8L,sample#9] json\n"
     ]
    }
   ],
   "source": [
    "# select candidates with opposite charge muons\n",
    "\n",
    "#d_flat = spark.sql(\"SELECT * FROM dimuon_df_table WHERE nMuon = 2 AND c1 == -c2\" )\n",
    "\n",
    "\n",
    "dimuon_flat = \n",
    "dimuon_flat.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76711812",
   "metadata": {},
   "source": [
    "To compute the invariant mass of the two candidate muons we need first to compute the four-momentum of the parent particle \n",
    "\n",
    "$$\n",
    "p = \\left(E_1+E_2,\\quad p_{x,1}+p_{x,2},\\quad p_{y,1}+p_{y,2},\\quad p_{z,1}+p_{z,2}\\right) = (E,\\, \\mathbf{p})\n",
    "$$\n",
    "\n",
    "Create four new columns in the dataframe, one per each coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f541cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns for the four momentum\n",
    "dimuon_flat = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367177bd",
   "metadata": {},
   "source": [
    "From the 4-momentum of the candidate we can compute the invariant mass as\n",
    "\n",
    "$$\n",
    "M = \\sqrt{p\\cdot p} =  \\sqrt{(E^2 - \\|\\mathbf{p}\\|^2)} = \\sqrt{(E^2 - (px^2 + py^2 + pz^2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute invariant mass\n",
    "from pyspark.sql.functions import sqrt\n",
    "\n",
    "dimuon_flat = \n",
    "\n",
    "# persist the dataframe in memory\n",
    "dimuon_flat = \n",
    "\n",
    "# show the first 5 elements of the mass (M) attribute\n",
    "dimuon_flat.select('M').show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f3cab",
   "metadata": {},
   "source": [
    "It is possible to obtain the same result using a UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92205706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import math\n",
    "\n",
    "# obtain the same result using udf\n",
    "#    create a UDF that takes as inputs E, px, py, pz and returns the invariant mass\n",
    "@udf('float')\n",
    "def invariant_mass(\"\"\" --- \"\"\"):\n",
    "    return \"\"\" --- \"\"\"\n",
    "\n",
    "# apply the UDF by simply calling it with the proper columns inputs (e.g. col('E') )\n",
    "dimuon_flat.select(\n",
    "    invariant_mass( \"\"\" --- \"\"\" )\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34c75a",
   "metadata": {},
   "source": [
    "<a href =\"#contents\">\n",
    "<p style=\"text-align: right;\">return to contents</p>\n",
    "</a>  \n",
    "<a id=\"udf\"></a>   \n",
    "\n",
    "## Pandas UDFs \n",
    "\n",
    "Similarly, we can use **Pandas UDFs**. \n",
    "\n",
    "This allows to process pyspark's dataframes as Pandas dataframes and Pandas series.\n",
    "\n",
    "They result particularly efficient in the grouped maps, i.e. by appling a function after a groupby. \n",
    "\n",
    "More on this can be found [here](https://spark.apache.org/docs/latest/api/python/user_guide/arrow_pandas.html#grouped-map). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# obtain the same result using a pandas udf\n",
    "#    create a UDF that takes as inputs the Pandas Series for E, px, py, pz \n",
    "#    and returns the Pandas Series for the invariant mass\n",
    "@pandas_udf('float')\n",
    "def invariant_mass_udf(\n",
    "    E: pd.Series,\n",
    "    \"\"\" ... \"\"\"\n",
    ") -> pd.Series:\n",
    "    return \"\"\" --- \"\"\"\n",
    "\n",
    "# apply the Pandas UDF \n",
    "#   rename the output by using the .alias(\"new name\") method\n",
    "dimuon_flat.select(\n",
    "    invariant_mass_udf( \"\"\" --- \"\"\").alias( \"\"\" --- \"\"\")\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75b03f",
   "metadata": {},
   "source": [
    "Compute the mean energy for different groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f38c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean energy for each sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c55b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the same results with a Pandas UDF \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d783a67",
   "metadata": {},
   "source": [
    "<a href =\"#contents\">\n",
    "<p style=\"text-align: right;\">return to contents</p>\n",
    "</a>  \n",
    "<a id=\"disco\"></a>   \n",
    "\n",
    "## (Re-)Discovering particles\n",
    "\n",
    "Now that all the interesting quantities have been computed we can use them to perform the analysis.\n",
    "\n",
    "For example, let's plot the invariant mass of the dimuon system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f85d34ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'M' given input columns: [E1, E2, c1, c2, nMuon, px1, px2, py1, py2, pz1, pz2, sample];\n'Project ['M]\n+- Project [sample#9, nMuon#8L, E1#491, px1#498, py1#506, pz1#515, c1#525L, E2#536, px2#548, py2#561, pz2#575, c2#590L]\n   +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, py1#506, pz1#515, c1#525L, E2#536, px2#548, py2#561, pz2#575, Muons[1]#486.charge AS c2#590L]\n      +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, py1#506, pz1#515, c1#525L, E2#536, px2#548, py2#561, Muons[1]#486.pz AS pz2#575]\n         +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, py1#506, pz1#515, c1#525L, E2#536, px2#548, Muons[1]#486.py AS py2#561]\n            +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, py1#506, pz1#515, c1#525L, E2#536, Muons[1]#486.px AS px2#548]\n               +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, py1#506, pz1#515, c1#525L, Muons[1]#486.E AS E2#536]\n                  +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, py1#506, pz1#515, Muons[0]#485.charge AS c1#525L]\n                     +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, py1#506, Muons[0]#485.pz AS pz1#515]\n                        +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, Muons[0]#485.py AS py1#506]\n                           +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, Muons[0]#485.px AS px1#498]\n                              +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, Muons[0]#485.E AS E1#491]\n                                 +- Project [sample#9, nMuon#8L, Muons#7[0] AS Muons[0]#485, Muons#7[1] AS Muons[1]#486]\n                                    +- Filter (nMuon#8L = cast(2 as bigint))\n                                       +- Relation [Muons#7,nMuon#8L,sample#9] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# compute the histogram of M\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m bins, counts \u001b[38;5;241m=\u001b[39m \u001b[43mdimuon_flat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m      3\u001b[0m                           \u001b[38;5;241m.\u001b[39mrdd \\\n\u001b[1;32m      4\u001b[0m                           \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mM) \\\n\u001b[1;32m      5\u001b[0m                           \u001b[38;5;241m.\u001b[39mhistogram(\u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m115\u001b[39m,\u001b[38;5;241m0.5\u001b[39m)))\n",
      "File \u001b[0;32m/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py:1685\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols):\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \n\u001b[1;32m   1667\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1685\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'M' given input columns: [E1, E2, c1, c2, nMuon, px1, px2, py1, py2, pz1, pz2, sample];\n'Project ['M]\n+- Project [sample#9, nMuon#8L, E1#491, px1#498, py1#506, pz1#515, c1#525L, E2#536, px2#548, py2#561, pz2#575, c2#590L]\n   +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, py1#506, pz1#515, c1#525L, E2#536, px2#548, py2#561, pz2#575, Muons[1]#486.charge AS c2#590L]\n      +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, py1#506, pz1#515, c1#525L, E2#536, px2#548, py2#561, Muons[1]#486.pz AS pz2#575]\n         +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, py1#506, pz1#515, c1#525L, E2#536, px2#548, Muons[1]#486.py AS py2#561]\n            +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, py1#506, pz1#515, c1#525L, E2#536, Muons[1]#486.px AS px2#548]\n               +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, py1#506, pz1#515, c1#525L, Muons[1]#486.E AS E2#536]\n                  +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, py1#506, pz1#515, Muons[0]#485.charge AS c1#525L]\n                     +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, py1#506, Muons[0]#485.pz AS pz1#515]\n                        +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, px1#498, Muons[0]#485.py AS py1#506]\n                           +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, E1#491, Muons[0]#485.px AS px1#498]\n                              +- Project [sample#9, nMuon#8L, Muons[0]#485, Muons[1]#486, Muons[0]#485.E AS E1#491]\n                                 +- Project [sample#9, nMuon#8L, Muons#7[0] AS Muons[0]#485, Muons#7[1] AS Muons[1]#486]\n                                    +- Filter (nMuon#8L = cast(2 as bigint))\n                                       +- Relation [Muons#7,nMuon#8L,sample#9] json\n"
     ]
    }
   ],
   "source": [
    "# compute the histogram of M\n",
    "bins, counts = dimuon_flat.select('M') \\\n",
    "                          .rdd \\\n",
    "                          .map(lambda x: x.M) \\\n",
    "                          .histogram(list(np.arange(0,115,0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd4905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute it for each sample\n",
    "histogram_results = {}\n",
    "for sample in ['mc', 'data']:\n",
    "    \"\"\" --- \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71712c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "mc_res = histogram_results['mc']\n",
    "bin_centers = mc_res['bins'][:-1] + np.diff(mc_res['bins'])/2\n",
    "plt.hist(\n",
    "    bin_centers, weights=mc_res['counts'], bins=mc_res['bins'],\n",
    "    label='Montecarlo', alpha=0.6\n",
    ")\n",
    "\n",
    "data_res = histogram_results['data']\n",
    "bin_centers = data_res['bins'][:-1] + np.diff(data_res['bins'])/2\n",
    "\n",
    "plt.errorbar(\n",
    "    bin_centers, data_res['counts'], yerr=np.sqrt(data_res['counts']),\n",
    "    fmt='o', ms=4, lw=1, color='black', label='data'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$m_{\\mu \\mu}$ [GeV]\")\n",
    "plt.ylabel(\"Events/0.5GeV\")\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1892c",
   "metadata": {},
   "source": [
    "In the range 70-120 GeV it appears there is a large resonance, we can histogram the data in that range to see it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_results = {}\n",
    "for sample in ['mc', 'data']:\n",
    "    histogram_results[sample] = {}\n",
    "    \n",
    "    # evaluate the new histograms here in the range 80-100\n",
    "    bins, counts = \"\"\" --- \"\"\"\n",
    "    \n",
    "    histogram_results[sample]['bins'] = bins\n",
    "    histogram_results[sample]['counts'] = counts\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "mc_res = histogram_results['mc']\n",
    "bin_centers = mc_res['bins'][:-1] + np.diff(mc_res['bins'])/2\n",
    "plt.hist(\n",
    "    bin_centers, weights=mc_res['counts'], bins=mc_res['bins'],\n",
    "    label='Montecarlo', alpha=0.6\n",
    ")\n",
    "\n",
    "data_res = histogram_results['data']\n",
    "bin_centers = data_res['bins'][:-1] + np.diff(data_res['bins'])/2\n",
    "\n",
    "plt.errorbar(\n",
    "    bin_centers, data_res['counts'], yerr=np.sqrt(data_res['counts']),\n",
    "    fmt='o', ms=4, lw=1, color='black', label='data'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$m_{\\mu \\mu}$ [GeV]\")\n",
    "plt.ylabel(\"Events/0.5GeV\")\n",
    "plt.legend()\n",
    "#plt.semilogy()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a7328",
   "metadata": {},
   "source": [
    "We can try to perform some selection, to improve the quality of the signal and remove background. To do this we can try look at candidates with a large transverse momentum \n",
    "\n",
    "$$\n",
    "p_T = \\sqrt{p_x^2 + p_y^2}\n",
    "$$\n",
    "\n",
    "create a colum with name \"energetic\" and for each row set the value to `True` if $p_T\\geq 30$ Gev. Plot then the results and see if there is any improvement. You can compare just the data or montecarlo. \n",
    "\n",
    "This can be achieved in spark using `when()` and `otherwise()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba342cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a udf to evaluate the dimuon candidate pT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591ecb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "dimuon_flat = dimuon_flat.withColumn(\"highpt\", when(dimuon_flat.highpt>30,True).otherwise(False) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb08d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimuon_flat['sample','M','highpt'].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d446d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_histogram(dataset, column):\n",
    "    histogram_results = {}\n",
    "    for sample in ['mc', 'data']:\n",
    "        histogram_results[sample] = {}\n",
    "\n",
    "        # create the invariant mass histogram in the range 70-115\n",
    "        bins, counts = \"\"\" --- \"\"\"\n",
    "\n",
    "        histogram_results[sample]['bins'] = bins\n",
    "        histogram_results[sample]['counts'] = counts\n",
    "        \n",
    "    return histogram_results\n",
    "        \n",
    "# compute two histograms:\n",
    "#   - one for the energetic (highpt) cadidates\n",
    "#   - another one for the non-energetic (not highpt) cadidates\n",
    "is_energetic = \"\"\" --- \"\"\"\n",
    "not_energetic = \"\"\" --- \"\"\"\n",
    " \n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "mc_res = not_energetic['mc']\n",
    "bin_centers = mc_res['bins'][:-1] + np.diff(mc_res['bins'])/2\n",
    "plt.hist(\n",
    "    bin_centers, weights=mc_res['counts'], bins=mc_res['bins'],\n",
    "    label='Not energetic', alpha=0.6\n",
    ")\n",
    "\n",
    "mc_res = is_energetic['mc']\n",
    "bin_centers = mc_res['bins'][:-1] + np.diff(mc_res['bins'])/2\n",
    "plt.hist(\n",
    "    bin_centers, weights=mc_res['counts'], bins=mc_res['bins'],\n",
    "    label='Energetic', alpha=0.6\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$m_{\\mu \\mu}$ [GeV]\")\n",
    "plt.ylabel(\"Events/0.5GeV\")\n",
    "plt.legend()\n",
    "#plt.semilogy()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53cb88c",
   "metadata": {},
   "source": [
    "## Stop worker and master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ffaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    # stop worker \n",
    "    $SPARK_HOME/sbin/stop-worker.sh\n",
    "    \n",
    "    # start master\n",
    "    $SPARK_HOME/sbin/stop-master.sh\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15eac15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
