{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4443b3",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "<center style =\"font-size: xx-large; font-weight: 600; line-height: 1.1;\">\n",
    "Lecture 3: Class Notes</center>  \n",
    "\n",
    "## Contents \n",
    "  \n",
    "* [Spark _Streaming_ context](#context)  \n",
    "* [Spark DataFrame](#c1)\n",
    "* [pyspark.pandas API](#c2)  \n",
    "* [Dimuon Example](#muon)  \n",
    "* [SQL like-view in spark, sql queries ect ](#sql)\n",
    "* [Pandas UDFs](#udf)\n",
    "* [(Re-)Discovering particles](#disco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40c3f9",
   "metadata": {},
   "source": [
    "# Lecture 3: Spark Streaming\n",
    "\n",
    "_Spark Streaming_ is an extension of the Spark API that enables scalabe stream processing. (through micro batching)\n",
    "\n",
    "The continous stream of input data can be ingested from many data sources such as **Kafka**, **Amazon s3** or **TCP sockets**. \n",
    "\n",
    "The Spark API allows to process data via high-level functions such as *map* and *reduce*. As we are going to see, it is also possible to use dataframe operations. \n",
    "\n",
    "Processed data can be exported to an external database and used to make live dashboards or offline analyses, or stored in files, or be used in a further stage of a Kafka pipeline. \n",
    "\n",
    "Spark streaming works by dividing the input data into _micro-batches_ that can be treated as static datasets. In Spark this is referred to as a *discretized stream* (*DStream*). The DStream is represented using RDDs.\n",
    "\n",
    "![DStream](imgs/lecture3/DStream.png)\n",
    "\n",
    "Any transformation applied on the DStream, i.e. anything like a `Dstream.map()`, will act independently on each batch. For example, in the image below, we can filter the original RDD to remove some data and produce a new stream. \n",
    "\n",
    "### Dstream: (discretized stream)dividing the input data into _micro-batches_ that can be treated as static datasets\n",
    "\n",
    "![DStream_filter](imgs/lecture3/Dstream_filter.png)\n",
    "\n",
    "In this lecture we will see how to setup a simple stream using a TCP socket as a data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77789660",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43dbb40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this variable with one of the following values\n",
    "# -> 'local'\n",
    "# -> 'docker_container'\n",
    "# -> 'docker_cluster'\n",
    "CLUSTER_TYPE ='docker_cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4330629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLUSTER_TYPE=docker_cluster\n"
     ]
    }
   ],
   "source": [
    "# set env variable\n",
    "%env CLUSTER_TYPE $CLUSTER_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ccf872",
   "metadata": {},
   "source": [
    "## Start the cluster \n",
    "\n",
    "Environment variables need to be set only in the case of a local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdc44b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLUSTER_TYPE=='local':\n",
    "    import findspark\n",
    "    findspark.init('/home/pazzini/work/courses/MAPD_B/MAPD-B/spark/spark-3.2.1-bin-hadoop3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1231432e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    echo \"Launching master and worker\"\n",
    "    \n",
    "    # start master \n",
    "    $SPARK_HOME/sbin/start-master.sh --host localhost \\\n",
    "        --port 7077 --webui-port 8080\n",
    "    \n",
    "    # start worker\n",
    "    $SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 \\\n",
    "        --cores 4 --memory 2g\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b0be3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\r\n",
      "\r\n",
      "#\r\n",
      "# Licensed to the Apache Software Foundation (ASF) under one or more\r\n",
      "# contributor license agreements.  See the NOTICE file distributed with\r\n",
      "# this work for additional information regarding copyright ownership.\r\n",
      "# The ASF licenses this file to You under the Apache License, Version 2.0\r\n",
      "# (the \"License\"); you may not use this file except in compliance with\r\n",
      "# the License.  You may obtain a copy of the License at\r\n",
      "#\r\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\r\n",
      "#\r\n",
      "# Unless required by applicable law or agreed to in writing, software\r\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      "# See the License for the specific language governing permissions and\r\n",
      "# limitations under the License.\r\n",
      "#\r\n",
      "\r\n",
      "# Starts a worker on the machine this script is executed on.\r\n",
      "#\r\n",
      "# Environment Variables\r\n",
      "#\r\n",
      "#   SPARK_WORKER_INSTANCES  The number of worker instances to run on this\r\n",
      "#                           worker.  Default is 1. Note it has been deprecate since Spark 3.0.\r\n",
      "#   SPARK_WORKER_PORT       The base port number for the first worker. If set,\r\n",
      "#                           subsequent workers will increment this number.  If\r\n",
      "#                           unset, Spark will find a valid port number, but\r\n",
      "#                           with no guarantee of a predictable pattern.\r\n",
      "#   SPARK_WORKER_WEBUI_PORT The base port for the web interface of the first\r\n",
      "#                           worker.  Subsequent workers will increment this\r\n",
      "#                           number.  Default is 8081.\r\n",
      "\r\n",
      "if [ -z \"${SPARK_HOME}\" ]; then\r\n",
      "  export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"\r\n",
      "fi\r\n",
      "\r\n",
      "# NOTE: This exact class name is matched downstream by SparkSubmit.\r\n",
      "# Any changes need to be reflected there.\r\n",
      "CLASS=\"org.apache.spark.deploy.worker.Worker\"\r\n",
      "\r\n",
      "if [[ $# -lt 1 ]] || [[ \"$@\" = *--help ]] || [[ \"$@\" = *-h ]]; then\r\n",
      "  echo \"Usage: ./sbin/start-worker.sh <master> [options]\"\r\n",
      "  pattern=\"Usage:\"\r\n",
      "  pattern+=\"\\|Using Spark's default log4j profile:\"\r\n",
      "  pattern+=\"\\|Started daemon with process name\"\r\n",
      "  pattern+=\"\\|Registered signal handler for\"\r\n",
      "\r\n",
      "  \"${SPARK_HOME}\"/bin/spark-class $CLASS --help 2>&1 | grep -v \"$pattern\" 1>&2\r\n",
      "  exit 1\r\n",
      "fi\r\n",
      "\r\n",
      ". \"${SPARK_HOME}/sbin/spark-config.sh\"\r\n",
      "\r\n",
      ". \"${SPARK_HOME}/bin/load-spark-env.sh\"\r\n",
      "\r\n",
      "# First argument should be the master; we need to store it aside because we may\r\n",
      "# need to insert arguments between it and the other arguments\r\n",
      "MASTER=$1\r\n",
      "shift\r\n",
      "\r\n",
      "# Determine desired worker port\r\n",
      "if [ \"$SPARK_WORKER_WEBUI_PORT\" = \"\" ]; then\r\n",
      "  SPARK_WORKER_WEBUI_PORT=8081\r\n",
      "fi\r\n",
      "\r\n",
      "# Start up the appropriate number of workers on this machine.\r\n",
      "# quick local function to start a worker\r\n",
      "function start_instance {\r\n",
      "  WORKER_NUM=$1\r\n",
      "  shift\r\n",
      "\r\n",
      "  if [ \"$SPARK_WORKER_PORT\" = \"\" ]; then\r\n",
      "    PORT_FLAG=\r\n",
      "    PORT_NUM=\r\n",
      "  else\r\n",
      "    PORT_FLAG=\"--port\"\r\n",
      "    PORT_NUM=$(( $SPARK_WORKER_PORT + $WORKER_NUM - 1 ))\r\n",
      "  fi\r\n",
      "  WEBUI_PORT=$(( $SPARK_WORKER_WEBUI_PORT + $WORKER_NUM - 1 ))\r\n",
      "\r\n",
      "  \"${SPARK_HOME}/sbin\"/spark-daemon.sh start $CLASS $WORKER_NUM \\\r\n",
      "     --webui-port \"$WEBUI_PORT\" $PORT_FLAG $PORT_NUM $MASTER \"$@\"\r\n",
      "}\r\n",
      "\r\n",
      "if [ \"$SPARK_WORKER_INSTANCES\" = \"\" ]; then\r\n",
      "  start_instance 1 \"$@\"\r\n",
      "else\r\n",
      "  for ((i=0; i<$SPARK_WORKER_INSTANCES; i++)); do\r\n",
      "    start_instance $(( 1 + $i )) \"$@\"\r\n",
      "  done\r\n",
      "fi\r\n"
     ]
    }
   ],
   "source": [
    "! cat $SPARK_HOME/sbin/start-worker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14513840",
   "metadata": {},
   "source": [
    "## Create the Spark session\n",
    "\n",
    "Later on we will explain what is the role of [Apache Arrow](), but first we need to install it and create the spark session with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86d10fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/bin/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/17 12:50:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://localhost:7077\")\\\n",
    "        .appName(\"Spark streaming application\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    \n",
    "    # use the provided master\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .appName(\"Spark streaming application\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "323d6601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://34cd3fcd6358:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark streaming application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-master:7077 appName=Spark streaming application>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b253c26",
   "metadata": {},
   "source": [
    "<a href =\"#contents\">\n",
    "<p style=\"text-align: right;\">return to contents</p>\n",
    "</a>  \n",
    "<a id=\"context\"></a>  \n",
    "\n",
    "\n",
    "## Spark _Streaming_ context\n",
    "\n",
    "The first step of a Spark streaming application is the creation of a `StreamingContext`. \n",
    "\n",
    "It is a similar concept to the `sparkContext` but it requires to be initialized with some additional information on how to handle the micro-batches.\n",
    "\n",
    "The context can be initialized using `StreamingContext(SparkContext, batch_interval`). \n",
    "The `batch_interval` parameter represents the wall-time between two batches, i.e. the batch duration in seconds.\n",
    "\n",
    "There could be **at most one** `StreamingContext` for each Spark application. \n",
    "\n",
    "The processing start when `StreamingContext.start()` is called, and it can be stopped with `StreamingContext.stop()`. \n",
    "\n",
    "By default, if the `StreamingContext` is stoped without passing the `stopSparkContext=False` option, the sparkContext is also stopped (thus the application is closed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "256cf2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# create a streaming context with a batch interval of 2 seconds\n",
    "ssc = StreamingContext(sc, 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922cfde8",
   "metadata": {},
   "source": [
    "For this example spark will read data from a TCP socket. \n",
    "\n",
    "A dummy data stream is generated by a simple python program that can be found in `utils/producer.py`. \n",
    "When executed, the producer will write data on port `5555` of `localhost`. \n",
    "\n",
    "The dataset consists of fake credit card transactions.\n",
    "\n",
    "Have a look at the code from the producer before executing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab8d890a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import socket\r\n",
      "import json\r\n",
      "import time\r\n",
      "import random\r\n",
      "import argparse\r\n",
      "\r\n",
      "first_names=('John','Andy','Joe','Alice')\r\n",
      "last_names=('Johnson','Smith','Jones', 'Millers')\r\n",
      "\r\n",
      "def send_messages(client_socket):\r\n",
      "    try:\r\n",
      "        while 1:\r\n",
      "            msg = {\r\n",
      "                'name': random.choice(first_names),\r\n",
      "                'surname': random.choice(last_names),\r\n",
      "                'amount': '{:.2f}'.format(random.random()*1000),\r\n",
      "                'delta_t': '{:.2f}'.format(random.random()*10),\r\n",
      "                'flag': random.choices([0,1], weights=[0.8, 0.2])[0]\r\n",
      "            }\r\n",
      "            client_socket.send((json.dumps(msg)+\"\\n\").encode('utf-8'))\r\n",
      "            time.sleep(0.1)\r\n",
      "\r\n",
      "    except KeyboardInterrupt:\r\n",
      "        exit()\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "    parser.add_argument('--hostname', type=str, required=True)\r\n",
      "    args = parser.parse_args()\r\n",
      "    print('Using hostname:', args.hostname)\r\n",
      "\r\n",
      "    new_skt = socket.socket()\r\n",
      "    host = args.hostname\r\n",
      "    port = 5555 \r\n",
      "    new_skt.bind((host, port))\r\n",
      "    print(\"Now listening on port: %s\" % str(port))\r\n",
      "\r\n",
      "    new_skt.listen(5) #  waiting for client connection.\r\n",
      "    c, addr = new_skt.accept()\r\n",
      "    print(\"Received request from: \" + str(addr))\r\n",
      "    # connection established, send messaged\r\n",
      "    send_messages(c)\r\n",
      "    \r\n"
     ]
    }
   ],
   "source": [
    "! cat utils/producer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a375f",
   "metadata": {},
   "source": [
    "The producer will generate new records in the form of a random combination of:\n",
    "- firstname\n",
    "- lastname\n",
    "- amount\n",
    "- delta time\n",
    "- flag\n",
    "\n",
    "These information will be formatted into a .json data format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d127aa0",
   "metadata": {},
   "source": [
    "To declare to Spark that the StreamingContext data source will be a TCP socket, located at a given `hostname` and `port`, we can use the following \n",
    "\n",
    "`socketTextStream(hostname, port)`\n",
    "\n",
    "Have a look at the StreamingContext documentation to see all the available options, at the [link](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b95ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a socket stream from the hostname where we collect data from\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    hostname = \"localhost\"\n",
    "    \n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    hostname = \"spark-master\"\n",
    "\n",
    "# and set the port to 5555\n",
    "socket_stream = ssc.socketTextStream(hostname, 5555)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad88b4",
   "metadata": {},
   "source": [
    "#### start the python producer.py script\n",
    "\n",
    "Depending on the Spark deployment mode for this exercise, do either of the following:\n",
    "\n",
    "**Local** \n",
    "- from a terminal, run the python script with the option `--hostname localhost`\n",
    "\n",
    "\n",
    "**Single Docker container**\n",
    "- from a terminal, identify the id of the container using `docker ps`\n",
    "- connect to the Docker container using the `docker exec -it <CONTAINER_ID> bash` command\n",
    "- from the docker container, run the python script with the option `--hostname localhost`\n",
    "  \n",
    "**Docker cluster**\n",
    "- from a terminal, identify the id of the `spark-master` container using `docker ps`\n",
    "- connect to the Docker container using the `docker exec -it <CONTAINER_ID> bash` command\n",
    "- from the docker container, run the python script with the option `--hostname spark-master`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685693cb",
   "metadata": {},
   "source": [
    "## Terminal\n",
    "```\n",
    "Last login: Tue May 17 09:44:08 2022 from 10.67.1.182\n",
    "(base) ubuntu@jj-vm:~$ docker ps\n",
    "CONTAINER ID   IMAGE                           COMMAND                  CREATED        STATUS       PORTS                                                                                  NAMES\n",
    "34cd3fcd6358   jpazzini/mapd-b:spark-jupyter   \"/bin/bash -c 'jupyt…\"   18 hours ago   Up 5 hours   0.0.0.0:4040->4040/tcp, :::4040->4040/tcp, 0.0.0.0:8888->8888/tcp, :::8888->8888/tcp   jupyterlab\n",
    "6ab12d9fe6b6   jpazzini/mapd-b:spark-worker    \"/bin/bash -c '${SPA…\"   18 hours ago   Up 5 hours                                                                                          spark_spark-worker_1\n",
    "ecd3501a38dd   jpazzini/mapd-b:spark-master    \"/bin/bash -c '${SPA…\"   18 hours ago   Up 5 hours   0.0.0.0:7077->7077/tcp, :::7077->7077/tcp, 0.0.0.0:8080->8080/tcp, :::8080->8080/tcp   spark-master\n",
    "(base) ubuntu@jj-vm:~$ docker exec -it spark-master bash\n",
    "(base) root@ecd3501a38dd:/# ls /opt/workspace/notebooks/utils/\n",
    "producer.py\n",
    "(base) root@ecd3501a38dd:/# python /opt/workspace/notebooks/utils/producer.py  --hostname spark-master\n",
    "Using hostname: spark-master\n",
    "Now listening on port: 5555\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163466ca",
   "metadata": {},
   "source": [
    "The first thing we need to to is load the json describing each transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3e7ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# use the map() transformation to apply the same function to all rdds\n",
    "# the function we want to run is the json.load() of the messages\n",
    "json_stream = socket_stream.map(lambda msg: json.loads(msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64806443",
   "metadata": {},
   "source": [
    "It is possible to print some elements of each batch with `pprint()`. This can be used to explore the RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49c29264",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stream.pprint() #not doing anything as ssc not actully assigned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0ad3b",
   "metadata": {},
   "source": [
    "Start the computations with `ssc.start()` and stop with `ssc.stop(stopSparkContext=False)`. Remember that after this last instruction the streaming context must be defined again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21738d69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                  (0 + 1) / 1][Stage 3:>                  (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3b52cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:15:38 WARN StreamingContext: StreamingContext has already been stopped\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(stopSparkContext=False) #in lec lost executer but should be okay "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7dc2a8",
   "metadata": {},
   "source": [
    "Now that we know we can stream data to Spark we would like to start performing basic operations in a distributed fashion.\n",
    "\n",
    "Once the SparkStreamingContext is stopped we have to re-create a new one, as the connection between the socket and Spark is lost.\n",
    "\n",
    "Before running the following cells we should therefore:\n",
    "1. create a new `scc` object\n",
    "2. point it to the proper TCP socket and port\n",
    "3. start again the python producer application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f1c082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#added in lecture \n",
    "# create a new Spark StreamingContext with a batch wall-time of 2 seconds\n",
    "ssc = StreamingContext(sc, 2) \n",
    "# create a socket stream from the hostname where we collect data from\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    hostname = \"localhost\"\n",
    "    \n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    hostname = \"spark-master\"\n",
    "\n",
    "# and set the port to 5555\n",
    "socket_stream = ssc.socketTextStream(hostname, 5555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c9ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the socket stream using the appropriate endpoint and port\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6894abd4",
   "metadata": {},
   "source": [
    "#### start once again the python producer script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86587a4a",
   "metadata": {},
   "source": [
    "We now start listening on the TCP socket, interpreting the input data stream as json loads.\n",
    "\n",
    "Remember to get rid of the `pprint()` action, that would otherwise be performed continously, dumping the input data into the Jupyter cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2a86004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new json_stream object by reading the json loads from the socket\n",
    "json_stream = socket_stream.map(lambda msg: json.loads(msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd0ef1c",
   "metadata": {},
   "source": [
    "create rows -> spark dataframe object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026ab834",
   "metadata": {},
   "source": [
    "We may want to convert each batch into a Spark DataFrame to have access to the higher level APIs. \n",
    "\n",
    "In order to do that, let's first convert all the numeric features of the json into python floats and integers variables. \n",
    "This is a simple type cast operation in python, that can be easily parallelized.\n",
    "\n",
    "The python dictionary produced by each json message, re-casted with the proper data types, can then be used to create a `Row`for each transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0782d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# create a row for each message \n",
    "#   convert each numerical value to the proper python type\n",
    "#   create a row from each message\n",
    "def create_row_rdd(t):\n",
    "    t['amount']+float(t['amount'])\n",
    "    t['delta_t']+float(t['delta_t'])\n",
    "    t['flag']+int(t['flag'])\n",
    "\n",
    "    return Row(**t)\n",
    "\n",
    "# apply the transformation to the json_stream rdd\n",
    "row_stream = json_stream.map(create_row_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceaa05f",
   "metadata": {},
   "source": [
    "The method `DStream.foreachRDD` can be used to apply custom transformations to each *batch* of data. \n",
    "\n",
    "In our case, we are insterested in converting each batch of data into a Spark dataframe and perform operations, such as finding the number of transactions for each user. \n",
    "\n",
    "For the scope of this simple use-case we can consider that all batches where a user has performed more than one transaction with the `flag` field equal to one can be identified as fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7224636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if left unconstrained, Spark might want to create a very \n",
    "# large number of partitions for this streaming application\n",
    "# \n",
    "# using way more partitions than necessary always results in\n",
    "# a huge over-head due to the partition-to-partition communications\n",
    "# \n",
    "# this line is a trick to force Spark to use a small number of partitions\n",
    "# thus making it more efficient in the case of small workloads and few executors\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 4) #might want 100 partitions, not idea with limited no of executors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a92c43",
   "metadata": {},
   "source": [
    "note: if a worker dies spark will recreate one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81cbd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit, countDistinct\n",
    "\n",
    "# process each bach to identify possibly fraudolent transactions\n",
    "#   1. convert the rdd into a dataframe (provide the schema if necessary)\n",
    "#   2. compute the number of transactions per batch per user (a unique combination of first_and_lastname)\n",
    "#   3. identify all the \"suspicios\" transactions by counting the number of transactions from a unique user \n",
    "#   3. print the resulting dataframe to mimick a live reporting of the suspicious transactions\n",
    "\n",
    "def process_batch(rdd):\n",
    "    # convert rdd to df\n",
    "    #   check the documentation and/or the Lecture2 notebook \n",
    "    #   for details on how to create and pass a schema to a dataframe   \n",
    "    df = row_stream.toDF(\n",
    "        schema = 'name string, surname string, amount float, delta_t float, flag int'\n",
    "    )\n",
    "    \n",
    "    # find number of transactions for each user when flag = 1 \n",
    "    #    declare a new column to create a unique user identifier \n",
    "    #    this can be easily done by concatenating first- and last-name fields\n",
    "    #    check the concat function from pyspark.sql.functions \n",
    "    num_transactions = (\n",
    "        df\n",
    "        .where(col('flag')==1)\n",
    "        .withColumn('id', concat(col('name'), col('surname')))\n",
    "        .groupBy('id')\n",
    "        .count()\n",
    "    )\n",
    "    \n",
    "    # find suspicious transactions\n",
    "    #    filter only users with more than one transaction per batch\n",
    "    #    create a \"fraud\" column with a value of 1 for the selected users (check the lit function)\n",
    "    #    from the dataframe, project only the unique id and fraud columns\n",
    "    \n",
    "    #could use pandas like synatx ie []\n",
    "    sus_transactions = (\n",
    "        num_transactions\n",
    "        .where(col('count')>1)\n",
    "        .withColumn('fraud', lit(1))\n",
    "        .select(col('id'),col('fraud'))\n",
    "        .count()\n",
    "    )\n",
    "    \n",
    "    # (trigger an automatic alert)\n",
    "    # print the first 5 items of the resulting dataframe\n",
    "    sus_transactions.show(5) #SPARK only reacts on the show all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a7758",
   "metadata": {},
   "source": [
    "Finally, instruct Spark to execute this `process_batch` function for each RDD you will have in your DStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b911a910",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.streaming.api.python.PythonDStream.callForeachRDD.\n: java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after stopping a context is not supported\n\tat org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:230)\n\tat org.apache.spark.streaming.dstream.DStream.<init>(DStream.scala:67)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.<init>(ForEachDStream.scala:39)\n\tat org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:655)\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$3(DStream.scala:640)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:792)\n\tat org.apache.spark.streaming.StreamingContext.withScope(StreamingContext.scala:264)\n\tat org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:640)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.callForeachRDD(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream.callForeachRDD(PythonDStream.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrow_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeachRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py:158\u001b[0m, in \u001b[0;36mDStream.foreachRDD\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    156\u001b[0m jfunc \u001b[38;5;241m=\u001b[39m TransformFunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc, func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer)\n\u001b[1;32m    157\u001b[0m api \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonDStream\n\u001b[0;32m--> 158\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallForeachRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjfunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.streaming.api.python.PythonDStream.callForeachRDD.\n: java.lang.IllegalStateException: Adding new inputs, transformations, and output operations after stopping a context is not supported\n\tat org.apache.spark.streaming.dstream.DStream.validateAtInit(DStream.scala:230)\n\tat org.apache.spark.streaming.dstream.DStream.<init>(DStream.scala:67)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.<init>(ForEachDStream.scala:39)\n\tat org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:655)\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$3(DStream.scala:640)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:792)\n\tat org.apache.spark.streaming.StreamingContext.withScope(StreamingContext.scala:264)\n\tat org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:640)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.callForeachRDD(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream.callForeachRDD(PythonDStream.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "row_stream.foreachRDD(process_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad010131",
   "metadata": {},
   "source": [
    "Now you should be ready to start the spark streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07b9ee85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1473.start.\n: java.lang.IllegalStateException: StreamingContext has already been stopped\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:615)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:557)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# start streaming context\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mssc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/context.py:187\u001b[0m, in \u001b[0;36mStreamingContext.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    Start the execution of the streams.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jssc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     StreamingContext\u001b[38;5;241m.\u001b[39m_activeContext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1473.start.\n: java.lang.IllegalStateException: StreamingContext has already been stopped\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:615)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:557)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# start streaming context\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b834a208",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:48:21 WARN StreamingContext: StreamingContext has already been stopped\n"
     ]
    }
   ],
   "source": [
    "# stop streaming context - no not stop the SparkContext\n",
    "ssc.stop(stopSparkContext=False) #in lec lost executer but should be okay "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d3e64",
   "metadata": {},
   "source": [
    "## Stop worker and master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ebe7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3fe580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    # stop worker \n",
    "    $SPARK_HOME/sbin/stop-worker.sh\n",
    "    \n",
    "    # start master\n",
    "    $SPARK_HOME/sbin/stop-master.sh\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
